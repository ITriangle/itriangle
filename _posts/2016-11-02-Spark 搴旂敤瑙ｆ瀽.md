---
layout: post
title: Spark 应用分析
categories: [Spark,大数据]
description: Spark 简单入门
keywords: 大数据,Saprk,Java
---

# Spark
深入分析一下 Spark 应用的处理流程.测试环境使用的

- spark-1.6.3-bin-hadoop2.6
- hadoop-2.6.0

## 实例分析

### 1.Maven依赖
针对前面实例使用的 Spark 2.0 API,出现了改动,如下更换为 1.6.3的API

```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.10</artifactId>
    <version>1.6.3</version>
</dependency>
```

### 2.分析文本中单词的个数

```java
public class Application {

    private static final Pattern SPACE = Pattern.compile(" ");

    public static void main(String[] args) throws Exception {

        //对于所有的 Spark 程序而言，要进行任何操作，首先要创建一个 Spark 的上下文，
        //在创建上下文的过程中，程序会向集群申请资源以及构建相应的运行环境。
        SparkConf sparkConf = new SparkConf().setAppName("JavaWordCount");
        JavaSparkContext ctx = new JavaSparkContext(sparkConf);

        //利用 textFile 接口从文件系统中读入指定的文件，返回一个 RDD 实例对象。
        //RDD 的初始创建都是由 SparkContext 来负责的，将内存中的集合或者外部文件系统作为输入源
        String logFile = "./wordcountdata.txt"; // Should be some file on your system
        
        JavaRDD<String> lines = ctx.textFile(logFile).cache();

        JavaRDD<String> words = lines.flatMap(
                new FlatMapFunction<String, String>() {
                    @Override
                    public Iterable<String> call(String s) throws Exception {
                        return Arrays.asList(SPACE.split(s));
                    }
                });

        JavaPairRDD<String, Integer> ones = words.mapToPair(
                new PairFunction<String, String, Integer>() {
                    @Override
                    public Tuple2<String, Integer> call(String s) {
                        return new Tuple2<String, Integer>(s, 1);
                    }
                });

        JavaPairRDD<String, Integer> counts = ones.reduceByKey(
                new Function2<Integer, Integer, Integer>() {
                    @Override
                    public Integer call(Integer i1, Integer i2) {
                        return i1 + i2;
                    }
                });

        List<Tuple2<String, Integer>> output = counts.collect();
        for (Tuple2<?, ?> tuple : output) {
            System.out.println(tuple._1() + ": " + tuple._2());
        }
        ctx.stop();
    }
}
```

样例代码来自于Spark Example.如果使用 Spark 2.0 API,则需要修改分词规则:

```java
/**
 * In 2.0, FlatMapFunction.call() returns an Iterator rather than Iterable
 */
@Override
public Iterable<String> call(String s) {
    return Arrays.asList(SPACE.split(s)).iterator();
}
```

### 3.流程解析

#### 1.创建 SparkContext 上下文
对于所有的 Spark 程序而言，要进行任何操作，首先要创建一个 Spark 的上下文，
在创建上下文的过程中，程序会向集群申请资源以及构建相应的运行环境。

#### 2.获取数据来源,创建某种数据类型的RDD
利用 textFile 接口从文件系统中读入指定的文件，返回一个 RDD 实例对象。RDD 的初始创建都是由 SparkContext 来负责的，将内存中的集合或者外部文件系统作为输入源.

#### 3.对RDD中的数据进行转换操作
map 函数，它根据现有的数据集返回一个新的分布式数据集，由每个原元素经过 func 函数转换后组成，这个过程一般叫做转换（transformation）;flatMap 函数类似于 map 函数，但是每一个输入元素，会被映射为 0 到多个输出元素，因此，func 函数的返回值是一个 Seq，而不是单一元素，可以从上面的代码中看出.

#### 4.对RDD执行操作
reduceByKey 函数在一个（K，V) 对的数据集上使用，返回一个（K，V）对的数据集，key 相同的值，都被使用指定的 reduce 函数聚合到一起。

## 说明
所有的 Spark 应用程序都离不开 SparkContext 和 Executor 两部分，Executor 负责执行任务，运行 Executor 的机器称为 Worker 节点，SparkContext 由用户程序启动，通过资源调度模块和 Executor 通信。SparkContext 和 Executor 这两部分的核心代码实现在各种运行模式中都是公用的，在它们之上，根据运行部署模式的不同，包装了不同调度模块以及相关的适配代码。具体来说，以 SparkContext 为程序运行的总入口，在 SparkContext 的初始化过程中，Spark 会分别创建 DAGScheduler 作业调度和 TaskScheduler 任务调度两极调度模块。其中，作业调度模块是基于任务阶段的高层调度模块，它为每个 Spark 作业计算具有依赖关系的多个调度阶段 (通常根据 Shuffle 来划分)，然后为每个阶段构建出一组具体的任务 (通常会考虑数据的本地性等)，然后以 TaskSets(任务组) 的形式提交给任务调度模块来具体执行。而任务调度模块则负责具体启动任务、监控和汇报任务运行情况。

总结一下Spark从开始到结果的运行过程：

1. 创建SparkContext上下文
2. 创建某种数据类型的RDD
3. 对RDD中的数据进行转换操作，例如过滤操作
4. 在需要重用的情况下，对转换后或过滤后的RDD进行缓存
5. 在RDD上进行action操作，例如提取数据、计数、存储数据到Cassandra等。

## 参考
1. [Spark 部署及示例代码讲解](https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-deploy1/)
2. [Apache Spark大数据分析入门（一）](http://www.csdn.net/article/2015-11-25/2826324)

