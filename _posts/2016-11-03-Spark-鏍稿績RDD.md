---
layout: post
title: Spark 核心 RDD
categories: [Spark,大数据]
description: RDD 弹性分布式数据集
keywords: 大数据,Spark,Java
---

# Spark RDD 在运行流程中作用
**Spark** 从开始到结果的运行过程:

1. 创建SparkContext上下文
2. 创建某种数据类型的RDD
3. 对RDD中的数据进行转换操作，例如过滤操作
4. 在需要重用的情况下，对转换后或过滤后的RDD进行缓存
5. 在RDD上进行action操作，例如提取数据、计数、存储数据到Cassandra等。

Spark在集群中可以并行地执行任务，并行度由Spark中的主要组件之一的RDD决定。弹性分布式数据集(Resilient distributed data, RDD)是一种数据表示方式，RDD中的数据被分区存储在集群中（碎片化的数据存储方式），正是由于数据的分区存储使得任务可以并行执行。分区数量越多，并行越高。下图给出了RDD的表示：
![](http://img.ptcms.csdn.net/article/201511/25/5655b0ea512a8.jpg)

想像每列均为一个分区（partition ），你可以非常方便地将分区数据分配给集群中的各个节点。

为创建RDD，可以从外部存储中读取数据，例如从Cassandra、Amazon简单存储服务（Amazon Simple Storage Service）、HDFS或其它Hadoop支持的输入数据格式中读取。也可以通过读取文件、数组或JSON格式的数据来创建RDD。另一方面，如果对于应用来说，数据是本地化的，此时你仅需要使用parallelize方法便可以将Spark的特性作用于相应数据，并通过Apache Spark集群对数据进行并行化分析。

## 一.RDD有何作用

对RDD，既可以进行数据转换，也可以对进行action操作。这意味着使用transformation可以改变数据格式、进行数据查询或数据过滤操作等，使用action操作，可以触发数据的改变、抽取数据、收集数据甚至进行计数。

例如，我们可以使用Spark中的文本文件 README.md 创建一个 RDD textFile，文件中包含了若干文本行，将该文本文件读入RDD textFile时，其中的文本行数据将被分区以便能够分发到集群中并被并行化操作。

得到的结果如下图所示：
![](http://img.ptcms.csdn.net/article/201511/25/5655b111f09fa.jpg)

然后，我们可以将所有包含Spark关键字的行筛选出来，完成操作后会生成一个新的:RDD linesWithSpark

在前一幅图中，我们给出了 textFile RDD的表示，下面的图为RDD linesWithSpark的表示：
![](http://img.ptcms.csdn.net/article/201511/25/5655b14b5a371.jpg)

值得注意的是，Spark还存在键值对RDD（Pair RDD），这种RDD的数据格式为键/值对数据（key/value paired data）。例如下表中的数据，它表示水果与颜色的对应关系：
![](http://img.ptcms.csdn.net/article/201511/25/5655b20626790.jpg)

对表中的数据使用groupByKey()转换操作将得到下列结果：
groupByKey() 转换操作

```
pairRDD.groupByKey()
Banana [Yellow]
Apple [Red, Green]      
Kiwi [Green]
Figs [Black]
```

该转换操作只将键为Apple，值为Red和Green的数据进行了分组。这些是到目前为止给出的转换操作例子。

### 2.RDD转化完成后可执行操作
当得到一个经过过滤操作后的RDD，可以collect/materialize相应的数据并使其流向应用程序，这是action操作的例子。经过此操作后， RDD中所有数据将消失，但我们仍然可以在RDD的数据上进行某些操作，因为它们仍然在内存当中。`linesWithSpark.collect()`

但每次进行Spark action操作时，例如count() action操作，Spark将重新启动所有的转换操作，计算将运行到最后一个转换操作，然后count操作返回计算结果，这种运行方式速度会较慢。为解决该问题和提高程序运行速度，可以将RDD的数据缓存到内存当中，这种方式的话，当你反复运行action操作时，能够避免每次计算都从头开始，直接从缓存到内存中的RDD得到相应的结果。`linesWithSpark.cache()`

如果你想将RDD linesWithSpark从缓存中清除，可以使用unpersist()方法。`linesWithSpark.unpersist()`.

如果不手动删除的话，在内存空间紧张的情况下，Spark会采用最近最久未使用（least recently used logic，LRU)调度算法删除缓存在内存中最久的RDD。

## 二.转化(transformation) 和 执行(action)

### 1. 转化

S.No | 转换&含义
---|---
1  | **map(func)** 返回一个新的分布式数据集，传递源的每个元素形成通过一个函数 func
2  | **filter(func)** 返回由选择在func返回true，源元素组成了一个新的数据集
3  | **flatMap(func)** 类似映射，但每个输入项目可以被映射到0以上输出项目(所以func应返回seq而不是单一的项目)
4  | **mapPartitions(func)** 类似映射，只不过是单独的每个分区(块)上运行RDD，因此 func 的类型必须是Iterator<T> ⇒ Iterator<U> 对类型T在RDD上运行时
5  | **mapPartitionsWithIndex(func)** 类似映射分区，而且还提供func 来表示分区的索引的整数值，因此 func 必须是类型 (Int, Iterator<T>) ⇒ Iterator<U> 当类型T在RDD上运行时
6  | **sample(withReplacement, fraction, seed)** 采样数据的一小部分，有或没有更换，利用给定的随机数发生器的种子
7  | **union(otherDataset)** 返回一个新的数据集，其中包含源数据和参数元素的结合
8  | **intersection(otherDataset)** 返回包含在源数据和参数元素的新RDD交集
9  | **distinct([numTasks])** 返回一个新的数据集包含源数据集的不同元素
10 | **groupByKey([numTasks])** 当调用(K，V)数据集，返回(K, Iterable<V>) 对数据集
11 | **reduceByKey(func, [numTasks])**
12 | **aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])**
13 | **sortByKey([ascending], [numTasks])**
14 | **join(otherDataset, [numTasks])**
15 | **cogroup(otherDataset, [numTasks])**
16 | **cartesian(otherDataset)** 当上调用类型T和U的数据集，返回(T，U)对数据集(所有元素对)
17 | **pipe(command, [envVars])** RDD通过shell命令每个分区，例如：一个Perl或bash脚本。RDD元素被写入到进程的标准输入和线路输出，标准输出形式返回一个字符串RDD
18 | **coalesce(numPartitions)** 减少RDD到numPartitions分区的数量。过滤大型数据集后，更高效地运行的操作
19 | **repartition(numPartitions)** 打乱RDD数据随机创造更多或更少的分区，并在它们之间平衡。这总是打乱的所有数据在网络上
20 | **repartitionAndSortWithinPartitions(partitioner)** 根据给定的分区重新分区RDD及在每个结果分区，排序键记录。这是调用重新分配排序在每个分区内，因为它可以推动分拣向下进入混洗机制效率更高。

### 2. 执行

S.No  | 操作&含义
---|---
1  | **reduce(func)** 合计数据集的元素，使用函数 func (其中有两个参数和返回一行). 该函数应该是可交换和可结合，以便它可以正确地在并行计算。
2  | **collect()** 返回数据集的所有作为数组在驱动程序的元素。这是一个过滤器或其它操作之后返回数据的一个足够小的子集，通常是有用的
3  | **count()** 返回该数据集的元素数
4  | **first()** 返回的数据集的第一个元素(类似于使用(1))
5  | **take(n)** 返回与该数据集的前n个元素的阵列。
6  | **takeSample (withReplacement,num, [seed])** 返回数组的数据集num个元素，有或没有更换随机抽样，预指定的随机数发生器的种子可选
7  | **takeOrdered(n, [ordering])** 返回RDD使用或者按其自然顺序或自定义比较的前第n个元素
8  | **saveAsTextFile(path)** 写入数据集是一个文本文件中的元素(或一组文本文件)，在给定的目录的本地文件系统，HDFS或任何其他的Hadoop支持的文件系统。Spark调用每个元素的 toString，将其转换为文件中的文本行
9  | **saveAsSequenceFile(path) (Java and Scala)**写入数据集，为Hadoop SequenceFile元素在给定的路径写入在本地文件系统，HDFS或任何其他Hadoop支持的文件系统。 这是适用于实现Hadoop可写接口RDDS的键 - 值对。在Scala中，它也可以在属于隐式转换为可写(Spark包括转换为基本类型，如 Int, Double, String 等等)类型。
10 | **saveAsObjectFile(path) (Java and Scala)** 写入数据集的内容使用Java序列化为一个简单的格式，然后可以使用SparkContext.objectFile()加载。
11 | **countByKey()** 仅适用于RDDS的类型 (K, V). 返回(K, Int)对与每个键的次数的一个HashMap。
12 | **foreach(func)** 数据集的每个元素上运行函数func。这通常对于不良反应，例如更新累加器或与外部存储系统进行交互进行。注 − 在 foreach()以外修改变量，其他累加器可能会导致不确定的行为。请参阅了解闭包的更多细节。

## 三.RDD




## 参考
1. [Apache Spark大数据分析入门（一）](http://www.csdn.net/article/2015-11-25/2826324)
2. [Spark API](http://spark.apache.org/docs/latest/programming-guide.html)
3. [理解Spark的核心RDD](http://www.infoq.com/cn/articles/spark-core-rdd)
4. [Spark Programming Guide](http://spark.apache.org/docs/latest/programming-guide.html)