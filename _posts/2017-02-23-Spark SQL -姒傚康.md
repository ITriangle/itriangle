---
layout: post
title: Spark SQL 一
categories: [Spark,大数据]
description: Spark SQL 作为 Spark 
keywords: 大数据,Spark,Java
---

# Spark SQL, DataFrames 以及 Datasets 编程指南

## 概要
Spark SQL是Spark中处理结构化数据的模块。与基础的Spark RDD API不同，Spark SQL的接口提供了更多关于数据的结构信息和计算任务的运行时信息。在Spark内部，Spark SQL会能够用于做优化的信息比RDD API更多一些。Spark SQL如今有了三种不同的API：SQL语句、DataFrame API和最新的Dataset API。不过真正运行计算的时候，无论你使用哪种API或语言，Spark SQL使用的执行引擎都是同一个。这种底层的统一，使开发者可以在不同的API之间来回切换，你可以选择一种最自然的方式，来表达你的需求。

本文中所有的示例都使用Spark发布版本中自带的示例数据，并且可以在spark-shell、pyspark shell以及sparkR shell中运行。

### SQL
Spark SQL的一种用法是直接执行SQL查询语句，你可使用最基本的SQL语法，也可以选择HiveQL语法。Spark SQL可以从已有的Hive中读取数据。更详细的请参考[Hive Tables](http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables) 这一节。如果用其他编程语言运行SQL，Spark SQL将以DataFrame返回结果。你还可以通过命令行[command-line](http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-spark-sql-cli) 或者 [JDBC/ODBC](http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server) 使用Spark SQL。 

### DataFrames
DataFrame是一种分布式数据集合，每一条数据都由几个命名字段组成。概念上来说，她和关系型数据库的表 或者 R和Python中的data frame等价，只不过在底层，DataFrame采用了更多优化。DataFrame可以从很多数据源（[sources](http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources)）加载数据并构造得到，如：结构化数据文件，Hive中的表，外部数据库，或者已有的RDD。

DataFrame API支持Scala, Java, Python, and R。

### Datasets
Dataset是Spark-1.6新增的一种API，目前还是实验性的。Dataset想要把RDD的优势（强类型，可以使用lambda表达式函数）和Spark SQL的优化执行引擎的优势结合到一起。Dataset可以由JVM对象构建（[constructed](http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-datasets) ）得到，而后Dataset上可以使用各种transformation算子（map，flatMap，filter 等）。

Dataset API 对 Scala 和 Java的支持接口是一致的，但目前还不支持Python，不过Python自身就有语言动态特性优势（例如，你可以使用字段名来访问数据，row.columnName）。对Python的完整支持在未来的版本会增加进来。

## 入门

### 起始点: SparkSession
Spark SQL所有的功能入口都是SQLContext 类，及其子类。不过要创建一个SQLContext对象，首先需要有一个SparkContext对象。

```java
JavaSparkContext sc = ...; // An existing JavaSparkContext.
SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);
```

除了SQLContext之外，你也可以创建HiveContext，HiveContext是SQLContext 的超集。

除了SQLContext的功能之外，HiveContext还提供了完整的HiveQL语法，UDF使用，以及对Hive表中数据的访问。要使用HiveContext，你并不需要安装Hive，而且SQLContext能用的数据源，HiveContext也一样能用。HiveContext是单独打包的，从而避免了在默认的Spark发布版本中包含所有的Hive依赖。如果这些依赖对你来说不是问题（不会造成依赖冲突等），建议你在Spark-1.3之前使用HiveContext。而后续的Spark版本，将会逐渐把SQLContext升级到和HiveContext功能差不多的状态。

spark.sql.dialect选项可以指定不同的SQL变种（或者叫SQL方言）。这个参数可以在SparkContext.setConf里指定，也可以通过 SQL语句的SET key=value命令指定。对于SQLContext，该配置目前唯一的可选值就是”sql”，这个变种使用一个Spark SQL自带的简易SQL解析器。而对于HiveContext，spark.sql.dialect 默认值为”hiveql”，当然你也可以将其值设回”sql”。仅就目前而言，HiveSQL解析器支持更加完整的SQL语法，所以大部分情况下，推荐使用HiveContext。

### 创建DataFrame
Spark应用可以用SparkContext创建DataFrame，所需的数据来源可以是已有的RDD（existing RDD），或者Hive表，或者其他数据源（data sources.）

以下是一个从JSON文件创建DataFrame的小栗子：

```java
JavaSparkContext sc = ...; // An existing JavaSparkContext.
SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);

DataFrame df = sqlContext.read().json("examples/src/main/resources/people.json");

// Displays the content of the DataFrame to stdout
df.show();
```


### DataFrame Operations
DataFrame提供了结构化数据的领域专用语言支持，包括Scala, Java, Python and R.

这里我们给出一个结构化数据处理的基本示例：

```java
JavaSparkContext sc // An existing SparkContext.
SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc)

// Create the DataFrame
DataFrame df = sqlContext.read().json("examples/src/main/resources/people.json");

// Show the content of the DataFrame
df.show();
// age  name
// null Michael
// 30   Andy
// 19   Justin

// Print the schema in a tree format
df.printSchema();
// root
// |-- age: long (nullable = true)
// |-- name: string (nullable = true)

// Select only the "name" column
df.select("name").show();
// name
// Michael
// Andy
// Justin

// Select everybody, but increment the age by 1
df.select(df.col("name"), df.col("age").plus(1)).show();
// name    (age + 1)
// Michael null
// Andy    31
// Justin  20

// Select people older than 21
df.filter(df.col("age").gt(21)).show();
// age name
// 30  Andy

// Count people by age
df.groupBy("age").count().show();
// age  count
// null 1
// 19   1
// 30   1
```

有关可在DataFrame上执行的操作类型的完整列表，请参阅[API文档](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/DataFrame.html)。

除了简单的列引用和表达式，DataFrames还有丰富的函数库，包括字符串操作，日期算术，常用的数学运算等。 完整的列表在[DataFrame函数参考](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/functions.html)中提供。

### 编程方式执行SQL查询(Running SQL Queries Programmatically)
SQLContext.sql可以执行一个SQL查询，并返回DataFrame结果。

```java
SQLContext sqlContext = ... // An existing SQLContext
DataFrame df = sqlContext.sql("SELECT * FROM table");
```


### 创建Dataset(Creating Datasets)
Dataset API和RDD类似，不过Dataset不使用Java序列化或者Kryo，而是使用专用的编码器（Encoder ）来序列化对象和跨网络传输通信。如果这个编码器和标准序列化都能把对象转字节，那么编码器就可以根据代码动态生成，并使用一种特殊数据格式，这种格式下的对象不需要反序列化回来，就能允许Spark进行操作，如过滤、排序、哈希等。

```java
JavaSparkContext sc = ...; // An existing JavaSparkContext.
SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);
```


### 和RDD互操作(Interoperating with RDDs)
Spark SQL有两种方法将RDD转为DataFrame。

1. 使用反射机制，推导包含指定类型对象RDD的schema。这种基于反射机制的方法使代码更简洁，而且如果你事先知道数据schema，推荐使用这种方式；

2. 编程方式构建一个schema，然后应用到指定RDD上。这种方式更啰嗦，但如果你事先不知道数据有哪些字段，或者数据schema是运行时读取进来的，那么你很可能需要用这种方式。

#### 利用反射推导schema
Spark SQL支持将JavaBeans的RDD自动转换为DataFrame。 使用反射获得的BeanInfo定义了表的模式。 目前，Spark SQL不支持包含嵌套或包含复杂类型（如列表或数组）的JavaBean。 您可以通过创建一个实现Serializable的类来创建一个JavaBean，并为其所有字段拥有getter和setter。

```java
public static class Person implements Serializable {
  private String name;
  private int age;

  public String getName() {
    return name;
  }

  public void setName(String name) {
    this.name = name;
  }

  public int getAge() {
    return age;
  }

  public void setAge(int age) {
    this.age = age;
  }
}
```

通过调用createDataFrame并为JavaBean提供Class对象，可以将模式应用于现有RDD。

```java
// sc is an existing JavaSparkContext.
SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);

// Load a text file and convert each line to a JavaBean.
JavaRDD<Person> people = sc.textFile("examples/src/main/resources/people.txt").map(
  new Function<String, Person>() {
    public Person call(String line) throws Exception {
      String[] parts = line.split(",");

      Person person = new Person();
      person.setName(parts[0]);
      person.setAge(Integer.parseInt(parts[1].trim()));

      return person;
    }
  });

// Apply a schema to an RDD of JavaBeans and register it as a table.
DataFrame schemaPeople = sqlContext.createDataFrame(people, Person.class);
schemaPeople.registerTempTable("people");

// SQL can be run over RDDs that have been registered as tables.
DataFrame teenagers = sqlContext.sql("SELECT name FROM people WHERE age >= 13 AND age <= 19")

// The results of SQL queries are DataFrames and support all the normal RDD operations.
// The columns of a row in the result can be accessed by ordinal.
List<String> teenagerNames = teenagers.javaRDD().map(new Function<Row, String>() {
  public String call(Row row) {
    return "Name: " + row.getString(0);
  }
}).collect();
```

#### 编程方式定义Schema
当JavaBean类不能提前定义时（例如，记录的结构被编码在一个字符串中，或者一个文本数据集将被解析，并且字段将被不同的用户投射），一个DataFrame可以用三个步骤 。

1. 从原始RDD创建行的RDD;
2. 创建由与第1步中创建的RDD中的Rows结构匹配的StructType表示的模式。
3. 通过SQLContext提供的createDataFrame方法将模式应用于Rows的RDD。

```java
import org.apache.spark.api.java.function.Function;
// Import factory methods provided by DataTypes.
import org.apache.spark.sql.types.DataTypes;
// Import StructType and StructField
import org.apache.spark.sql.types.StructType;
import org.apache.spark.sql.types.StructField;
// Import Row.
import org.apache.spark.sql.Row;
// Import RowFactory.
import org.apache.spark.sql.RowFactory;

// sc is an existing JavaSparkContext.
SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);

// Load a text file and convert each line to a JavaBean.
JavaRDD<String> people = sc.textFile("examples/src/main/resources/people.txt");

// The schema is encoded in a string
String schemaString = "name age";

// Generate the schema based on the string of schema
List<StructField> fields = new ArrayList<StructField>();
for (String fieldName: schemaString.split(" ")) {
  fields.add(DataTypes.createStructField(fieldName, DataTypes.StringType, true));
}
StructType schema = DataTypes.createStructType(fields);

// Convert records of the RDD (people) to Rows.
JavaRDD<Row> rowRDD = people.map(
  new Function<String, Row>() {
    public Row call(String record) throws Exception {
      String[] fields = record.split(",");
      return RowFactory.create(fields[0], fields[1].trim());
    }
  });

// Apply the schema to the RDD.
DataFrame peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema);

// Register the DataFrame as a table.
peopleDataFrame.registerTempTable("people");

// SQL can be run over RDDs that have been registered as tables.
DataFrame results = sqlContext.sql("SELECT name FROM people");

// The results of SQL queries are DataFrames and support all the normal RDD operations.
// The columns of a row in the result can be accessed by ordinal.
List<String> names = results.javaRDD().map(new Function<Row, String>() {
  public String call(Row row) {
    return "Name: " + row.getString(0);
  }
}).collect();
```


## 参考
1. [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/1.6.1/sql-programming-guide.html#creating-dataframes)