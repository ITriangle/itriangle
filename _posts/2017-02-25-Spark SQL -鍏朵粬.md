---
layout: post
title: Spark SQL 三
categories: [Spark,大数据]
description: Spark SQL 作为 Spark 
keywords: 大数据,Spark,Java
---

## 性能调整(Performance Tuning)
对于有一定计算量的Spark作业来说，可能的性能改进的方式，不是把数据缓存在内存里，就是调整一些开销较大的选项参数。

### 内存缓存(Caching Data In Memory)
Spark SQL可以通过调用SQLContext.cacheTable(“tableName”)或者DataFrame.cache()把tables以列存储格式缓存到内存中。随后，Spark SQL将会扫描必要的列，并自动调整压缩比例，以减少内存占用和GC压力。你也可以用SQLContext.uncacheTable(“tableName”)来删除内存中的table。

还可以使用SQLContext.setConf 或在SQL语句中运行SET key=value命令，来配置内存中的缓存。

属性名| 默认值| 含义
---|---|---
spark.sql.inMemoryColumnarStorage.compressed|  true|  如果设置为true，Spark SQL将会根据数据统计信息，自动为每一列选择单独的压缩编码方式。
spark.sql.inMemoryColumnarStorage.batchSize| 10000| 控制列式缓存批量的大小。增大批量大小可以提高内存利用率和压缩率，但同时也会带来OOM（Out Of Memory）的风险。

### 其他配置选项(Other Configuration Options)
以下选项同样也可以用来给查询任务调性能。不过这些选项在未来可能被放弃，因为spark将支持越来越多的自动优化。

属性名| 默认值| 含义
---|---|---
spark.sql.autoBroadcastJoinThreshold|  10485760 (10 MB) | 配置join操作时，能够作为广播变量的最大table的大小。设置为-1，表示禁用广播。注意，目前的元数据统计仅支持Hive metastore中的表，并且需要运行这个命令：ANALYSE TABLE <tableName> COMPUTE STATISTICS noscan
spark.sql.tungsten.enabled|  true|  设为true，则启用优化的Tungsten物理执行后端。Tungsten会显式的管理内存，并动态生成表达式求值的字节码
spark.sql.shuffle.partitions | 200| 配置数据混洗（shuffle）时（join或者聚合操作），使用的分区数。

## 分布式SQL引擎(Distributed SQL Engine)
Spark SQL可以作为JDBC/ODBC或者命令行工具的分布式查询引擎。在这种模式下，终端用户或应用程序，无需写任何代码，就可以直接在Spark SQL中运行SQL查询。

### 运行Thrift JDBC/ODBC server(Running the Thrift JDBC/ODBC server)
这里实现的Thrift JDBC/ODBC server和Hive-1.2.1中的HiveServer2是相同的。你可以使用beeline脚本来测试Spark或者Hive-1.2.1的JDBC server。

在Spark目录下运行下面这个命令，启动一个JDBC/ODBC server:`./sbin/start-thriftserver.sh`

这个脚本能接受所有 bin/spark-submit 命令支持的选项参数，外加一个 –hiveconf 选项，来指定Hive属性。运行./sbin/start-thriftserver.sh –help可以查看完整的选项列表。默认情况下，启动的server将会在localhost:10000端口上监听。要改变监听主机名或端口，可以用以下环境变量：

```
export HIVE_SERVER2_THRIFT_PORT=<listening-port>
export HIVE_SERVER2_THRIFT_BIND_HOST=<listening-host>
./sbin/start-thriftserver.sh \
  --master <master-uri> \
  ...
```

或者Hive系统属性 来指定

```
./sbin/start-thriftserver.sh \
  --hiveconf hive.server2.thrift.port=<listening-port> \
  --hiveconf hive.server2.thrift.bind.host=<listening-host> \
  --master <master-uri>
  ...
```

接下来，你就可以开始在beeline中测试这个Thrift JDBC/ODBC server:

```
./bin/beeline
```

下面的指令，可以连接到一个JDBC/ODBC server

```
beeline> !connect jdbc:hive2://localhost:10000
```

可能需要输入用户名和密码。在非安全模式下，只要输入你本机的用户名和一个空密码即可。对于安全模式，请参考beeline documentation.

Hive的配置是在conf/目录下的hive-site.xml，core-site.xml，hdfs-site.xml中指定的。

你也可以在beeline的脚本中指定。

Thrift JDBC server也支持通过HTTP传输Thrift RPC消息。以下配置（在conf/hive-site.xml中）将启用HTTP模式：

```
hive.server2.transport.mode - Set this to value: http
hive.server2.thrift.http.port - HTTP port number fo listen on; default is 10001
hive.server2.http.endpoint - HTTP endpoint; default is cliservice
```

同样，在beeline中也可以用HTTP模式连接JDBC/ODBC server:

```
beeline> !connect jdbc:hive2://<host>:<port>/<database>?hive.server2.transport.mode=http;hive.server2.thrift.http.path=<http_endpoint>
```

### 使用Spark SQL命令行工具(Running the Spark SQL CLI)
Spark SQL CLI是一个很方便的工具，它可以用local mode运行hive metastore service，并且在命令行中执行输入的查询。注意Spark SQL CLI目前还不支持和Thrift JDBC server通信。

用如下命令，在spark目录下启动一个Spark SQL CLI

```
./bin/spark-sql
```

Hive配置在conf目录下hive-site.xml，core-site.xml，hdfs-site.xml中设置。你可以用这个命令查看完整的选项列表：`./bin/spark-sql –help`

## 参考(Reference)

### 数据类型(Data Types)
Spark SQL和DataFrames支持如下数据类型：

1. Numeric types（数值类型）
    + ByteType: 1字节长的有符号整型，范围：-128 到 127.
    + ShortType: 2字节长有符号整型，范围：-32768 到 32767.
    + IntegerType: 4字节有符号整型，范围：-2147483648 到 2147483647.
    + LongType: 8字节有符号整型，范围： -9223372036854775808 to 9223372036854775807.
    + FloatType: 4字节单精度浮点数。
    + DoubleType: 8字节双精度浮点数
    + DecimalType: 任意精度有符号带小数的数值。内部使用java.math.BigDecimal,BigDecimal包含任意精度的不缩放整型，和一个32位的缩放整型
2. String type（字符串类型）:
    - StringType: 字符串
3. Binary type（二进制类型）:
    - BinaryType: 字节序列
4. Boolean type（布尔类型）:
    - BooleanType: 布尔类型
5. Datetime type（日期类型）
    - TimestampType: 表示包含年月日、时分秒等字段的日期
    - DateType: 表示包含年月日字段的日期
6. Complex types（复杂类型）
    - ArrayType(elementType, containsNull)：数组类型，表达一系列的elementType类型的元素组成的序列，containsNull表示数组能否包含null值
    - MapType(keyType, valueType, valueContainsNull)：映射集合类型，表示一个键值对的集合。键的类型是keyType，值的类型则由valueType指定。对应MapType来说，键是不能为null的，而值能否为null则取决于valueContainsNull。
    - StructType(fields)：表示包含StructField序列的结构体。
        - StructField(name, datatype, nullable): 表示StructType中的一个字段，name是字段名，datatype是数据类型，nullable表示该字段是否可以为空

Spark SQL的所有数据类型都位于org.apache.spark.sql.types的包中。要访问或创建数据类型，请使用org.apache.spark.sql.types.DataTypes中提供的工厂方法。

Data type| Value type in Scala |API to access or create a data type
---|---|---
ByteType|Byte|ByteType
ShortType|Short|ShortType
IntegerType|Int|IntegerType
LongType| Long| LongType
FloatType|Float|FloatType
DoubleType| Double| DoubleType
DecimalType|java.math.BigDecimal| DecimalType
StringType| String| StringType
BinaryType| Array[Byte]|BinaryType
BooleanType|Boolean|BooleanType
TimestampType|java.sql.Timestamp| TimestampType
DateType| java.sql.Date|DateType
ArrayType|scala.collection.Seq| ArrayType(elementType, [containsNull])注意：默认containsNull为true
MapType|scala.collection.Map|  MapType(keyType, valueType, [valueContainsNull])注意：默认valueContainsNull为true
StructType|  org.apache.spark.sql.Row|  StructType(fields)注意：fields是一个StructFields的序列，并且同名的字段是不允许的。
StructField| 定义字段的数据对应的Scala类型（例如，如果StructField的dataType为IntegerType，则其数据对应的scala类型为Int）| StructField(name, dataType, nullable)

### NaN Semantics
这是Not-a-Number的缩写，某些float或double类型不符合标准浮点数语义，需要对其特殊处理：

- NaN == NaN，即：NaN和NaN总是相等
- 在聚合函数中，所有NaN分到同一组
- NaN在join操作中可以当做一个普通的join key
- NaN在升序排序中排到最后，比任何其他数值都大

## 参考
1. [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/1.6.1/sql-programming-guide.html#creating-dataframes)
2. [用Apache Spark进行大数据处理——第二部分：Spark SQL](http://www.infoq.com/cn/articles/apache-spark-sql)
3. [Spark 实战，第 3 部分: 使用 Spark SQL 对结构化数据进行统计分析](https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-practice3/)
4. [Spark SQL, DataFrames 以及 Datasets 编程指南](http://ifeve.com/spark-sql-dataframes/)