---
layout: post
title: Spark 入门
categories: [Spark,大数据]
description: Spark 简单入门
keywords: 大数据,Saprk,Java
---

# Spark
Apache Spark生态系统中的包和框架日益丰富，使得Spark能够进行高级数据分析。Apache Spark的快速成功得益于它的强大功能和易于使用性。相比于传统的MapReduce大数据分析，Spark效率更高、运行时速度更快。Apache Spark 提供了内存中的分布式计算能力，具有Java、 Scala、Python、R四种编程语言的API编程接口。Spark生态系统如下图所示：
![](http://img.ptcms.csdn.net/article/201511/25/5655a8619e7c9.jpg)

## 一.Spark 组件

1. **Apache Spark 核心** :是底层一般执行引擎，所有其他的功能都是建立在Spark的平台之上。 它提供了内存计算和引用数据集在外部存储系统。
2. **Spark SQL** :是引入了一种名为SchemaRDD一个新的数据抽象，它提供了结构化和半结构化数据的支持，它是Spark核心的组成部分。
3. **Spark 数据流** : Spark数据流充分利用Spark核心快速调度能力进行流分析。它摄取的数据在小型分批进行RDD（弹性分布式数据集），对这些小批量的数据转换。
4. **MLlib (机器学习库)** : MLlib是一个分布式的机器学习框架的分布式存储器为基础的Spark架构。它根据基准，由MLlib开发商对交替最小二乘（ALS）实现完成。Spark MLlib 9倍快于Hadoop基于磁盘的版本（前Mahout中获得了Spark接口）Apache Mahout 。
5. **GraphX** :GraphX是Spark顶部上的分布式图形处理框架。它提供了用于表达图表计算，并可以通过使用预凝胶抽象API来建模用户定义的图形的API。它还提供了这种抽象的优化运行

## 二.Spark 建立
Spark在Hadoop上建立,下图显示了Spark 如何使用Hadoop组件的三种方式来构建。
![](http://www.yiibai.com/uploads/tutorial/20151117/1-15111F64425911.jpg)

有三种方法部署Spark ,如下面所解释:

1. **单机版** − Spark独立部署是指Spark占据在HDFS之上（Hadoop分布式文件系统）并将空间分配给HDFS。在这里，Spark和MapReduce将并列覆盖所有Spark的作业集群。
2. **Hadoop Yarn** − Hadoop Yarn部署方式，简单地说，spark运行在Yarn没有任何必要预安装或使用root访问权限。它有助于Spark融入Hadoop生态系统和Hadoop堆栈。它允许在其它部件叠上层的顶部上运行。
3. **Spark 在 MapReduce (SIMR)**:− Spark在MapReduce的用于启动spark作业，除了独立部署。通过SIMR，用户可以启动Spark和使用Shell，而不需要任何管理权限。

## 三.Spark 安装
略,具体查看链接!

## 四.Spark 测试样例

### 1.Maven依赖

```xml
<dependency> <!-- Spark dependency -->
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.11</artifactId>
    <version>2.1.0</version>
</dependency>
```

### 2.样例代码

```java
import org.apache.spark.api.java.*;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.Function;

public class Application {

    private static final Pattern SPACE = Pattern.compile(" ");

    public static void main(String[] args) {
        String logFile = "./wordcountdata.txt"; // Should be some file on your system
        SparkConf sparkConf = new SparkConf().setAppName("Simple Application");
        JavaSparkContext ctx = new JavaSparkContext(sparkConf);
        JavaRDD<String> lines = ctx.textFile(logFile).cache();

        long numAs = lines.filter(new Function<String, Boolean>() {
            public Boolean call(String s) { return s.contains("a"); }
        }).count();

        long numBs = lines.filter(new Function<String, Boolean>() {
            public Boolean call(String s) { return s.contains("b"); }
        }).count();

        System.out.println("Lines with a: " + numAs + ", lines with b: " + numBs);

        ctx.stop();


    }
}
```

运行时,需要设置项的`Configuration`中的`VM opthion` 增加`-Dspark.master=local`.不然会报 `Spark` 初始化错误.

### 3.未设置 Spark

在运行Spark的测试程序SparkPi时，点击运行，出现了如下错误：

```
Exception in thread "main" org.apache.spark.SparkException: A master URL must be set in your configuration
```

从提示中可以看出找不到程序运行的master，此时需要配置环境变量。
传递给spark的master url可以有如下几种：

- **local** 本地单线程
- **local[K]** 本地多线程（指定K个内核）
- **local[*]** 本地多线程（指定所有可用内核）
- **spark://HOST:PORT** 连接到指定的 Spark standalone cluster master，需要指定端口。
- **mesos://HOST:PORT** 连接到指定的 Mesos 集群，需要指定端口。
- **yarn-client客户端模式** 连接到 YARN 集群。需要配置 HADOOP_CONF_DIR。
- **yarn-cluster集群模式** 连接到 YARN 集群。需要配置 HADOOP_CONF_DIR。

## 参考
1. [单机安装Spark开发环境](http://www.cnblogs.com/eczhou/p/5216918.html)
2. [Spark 部署及示例代码讲解](https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-deploy1/)
3. [ spark (java API) 在Intellij IDEA中开发并运行](http://blog.csdn.net/dream_an/article/details/54915894)
4. [Apache Spark大数据分析入门（一）](http://www.csdn.net/article/2015-11-25/2826324)
5. [Spark教程](http://www.yiibai.com/spark/)
6. [理解Spark的核心RDD](http://www.infoq.com/cn/articles/spark-core-rdd)